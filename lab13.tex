\documentclass{article}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[margin=1.0in]{geometry} % To set margins
\usepackage{amsmath}  % This allows me to use the align functionality.
                      % If you find yourself trying to replicate
                      % something you found online, ensure you're
                      % loading the necessary packages!
\usepackage{amsfonts} % Math font
\usepackage{fancyvrb}
\usepackage{hyperref} % For including hyperlinks
\usepackage[shortlabels]{enumitem}% For enumerated lists with labels specified
                                  % We had to run tlmgr_install("enumitem") in R
\usepackage{float}    % For telling R where to put a table/figure
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item When conducting the work of Lab 11, we conducted the test that uses the
Central Limit Theorem even though the sample size was ``small" (i.e., $n<30$).
It turns out, that how ``far off" the $t$-test is can be computed using
a first-order Edgeworth approximation for the error. Below, we will do this 
for the the further observations.
\begin{enumerate}
  \item \cite{Boos00} note that 
  \begin{align*}
    P(T \leq t) \approx F_Z(t) + \underbrace{\frac{\text{skew}}{\sqrt{n}} \frac{(2t^2+1)}{6} f_Z(t)}_{\textrm{error}},
  \end{align*}
  where $f_Z(\cdot)$ and $F_Z(\cdot)$ are the Gaussian PDF and CDF and skew is the
  skewness of the data. What is the potential error in the computation of the 
  $p$-value when testing $H_0: \mu_X=0; H_a: \mu_X<0$ using the zebra finch further data? 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hldef{(tidyverse)}
\hlcom{#Load in Data}
\hldef{dat.finch} \hlkwb{=} \hlkwd{read.csv}\hldef{(}\hlsng{"zebrafinches.csv"}\hldef{)}

\hlcom{#Question 1}
\hlkwd{library}\hldef{(moments)}\hlcom{#used for calculating statistics}
\hlkwd{library}\hldef{(ggplot2)}

\hldef{n} \hlkwb{<-} \hlkwd{length}\hldef{(dat.finch}\hlopt{$}\hldef{further)}
\hldef{x_bar} \hlkwb{<-} \hlkwd{mean}\hldef{(dat.finch}\hlopt{$}\hldef{further)}
\hldef{s} \hlkwb{<-} \hlkwd{sd}\hldef{(dat.finch}\hlopt{$}\hldef{further)}
\hldef{t_obs} \hlkwb{<-} \hldef{x_bar} \hlopt{/} \hldef{(s} \hlopt{/} \hlkwd{sqrt}\hldef{(n))} \hlcom{#t-value}
\hldef{skew} \hlkwb{<-} \hlkwd{skewness}\hldef{(dat.finch}\hlopt{$}\hldef{further)}

\hlcom{#Could use this instead shows same thing}
\hlcom{#(ttest <- t.test(x = dat.finch$further,}
                \hlcom{# mu = 0,}
                \hlcom{# alternative = "less")) }
\hlcom{# Gaussian PDF and CDF at t}
\hldef{fz} \hlkwb{<-} \hlkwd{dnorm}\hldef{(t_obs)}
\hldef{Fz} \hlkwb{<-} \hlkwd{pnorm}\hldef{(t_obs)}

\hlcom{# Edgeworth approximation error}
\hldef{edgeworth_error} \hlkwb{<-} \hldef{(skew} \hlopt{/} \hlkwd{sqrt}\hldef{(n))} \hlopt{*} \hldef{((}\hlnum{2} \hlopt{*} \hldef{t_obs}\hlopt{^}\hlnum{2} \hlopt{+} \hlnum{1}\hldef{)} \hlopt{/} \hlnum{6}\hldef{)} \hlopt{*} \hldef{fz}
\end{alltt}
\end{kframe}
\end{knitrout}
The Edgeworth error in p-value estimate: -1.303424e-13. This means the potential error is very small only changing the p value by 1.303424e-11 percent which is not important. 
  \item Compute the error for $t$ statistics from -10 to 10 and plot a line
  that shows the error across $t$. Continue to use the skewness and 
  the sample size for the zebra finch further data.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{t_vals} \hlkwb{<-} \hlkwd{seq}\hldef{(}\hlopt{-}\hlnum{10}\hldef{,} \hlnum{10}\hldef{,} \hlkwc{length.out} \hldef{=} \hlnum{1000}\hldef{)}
\hldef{fz_vals} \hlkwb{<-} \hlkwd{dnorm}\hldef{(t_vals)}
\hldef{error_vals} \hlkwb{<-} \hldef{(skew} \hlopt{/} \hlkwd{sqrt}\hldef{(n))} \hlopt{*} \hldef{((}\hlnum{2} \hlopt{*} \hldef{t_vals}\hlopt{^}\hlnum{2} \hlopt{+} \hlnum{1}\hldef{)} \hlopt{/} \hlnum{6}\hldef{)} \hlopt{*} \hldef{fz_vals}

\hldef{error_df} \hlkwb{<-} \hlkwd{data.frame}\hldef{(}\hlkwc{t} \hldef{= t_vals,} \hlkwc{error} \hldef{= error_vals)}

\hlkwd{ggplot}\hldef{(error_df,} \hlkwd{aes}\hldef{(}\hlkwc{x} \hldef{= t,} \hlkwc{y} \hldef{= error))} \hlopt{+}
  \hlkwd{geom_line}\hldef{(}\hlkwc{color} \hldef{=} \hlsng{"blue"}\hldef{)} \hlopt{+}
  \hlkwd{labs}\hldef{(}\hlkwc{title} \hldef{=} \hlsng{"Edgeworth Approximation Error across t-values"}\hldef{,}
       \hlkwc{x} \hldef{=} \hlsng{"t"}\hldef{,} \hlkwc{y} \hldef{=} \hlsng{"Error in P(T <= t)"}\hldef{)} \hlopt{+}
  \hlkwd{theme_minimal}\hldef{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-1} 
\end{knitrout}
  \item Suppose we wanted to have a tail probability within 10\% of the desired
  $\alpha=0.05$. Recall we did a left-tailed test using the further data.
  How large of a sample size would we need? That is, we need
  to solve the error formula equal to 10\% of the desired left-tail probability:
  \[0.10 \alpha  \stackrel{set}{=} \underbrace{\frac{\text{skew}}{\sqrt{n}} \frac{(2t^2+1)}{6} f_Z(t)}_{\textrm{error}},\]
  which yields
  \[ n = \left(\frac{\text{skew}}{6(0.10\alpha)} (2t^2 + 1) f_Z(t)\right)^2.\]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{alpha} \hlkwb{<-} \hlnum{0.05}
\hldef{target_error} \hlkwb{<-} \hlnum{0.10} \hlopt{*} \hldef{alpha}  \hlcom{# 10% of alpha}
\hldef{t_alpha} \hlkwb{<-} \hlkwd{qnorm}\hldef{(alpha)}  \hlcom{# for left-tailed test}
\hldef{fz_alpha} \hlkwb{<-} \hlkwd{dnorm}\hldef{(t_alpha)}

\hlcom{# Solve for n}
\hldef{numerator} \hlkwb{<-} \hldef{skew} \hlopt{*} \hldef{(}\hlnum{2} \hlopt{*} \hldef{t_alpha}\hlopt{^}\hlnum{2} \hlopt{+} \hlnum{1}\hldef{)} \hlopt{*} \hldef{fz_alpha}
\hldef{n_required} \hlkwb{<-} \hldef{(numerator} \hlopt{/} \hldef{(}\hlnum{6} \hlopt{*} \hldef{target_error))}\hlopt{^}\hlnum{2}
\end{alltt}
\end{kframe}
\end{knitrout}
The required sample size to keep the Edgeworth approximation error within 10\% of the tail probability $\alpha = 0.05$ is approximately $n = 589$.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Complete the following steps to revisit the analyses from lab 11 using the
bootstrap procedure.
\begin{enumerate}
\item Now, consider the zebra finch data. We do not know the generating distributions
for the closer, further, and difference data, so perform resampling to approximate the 
sampling distribution of the $T$ statistic:
  \[T = \frac{\bar{x}_r - 0}{s/\sqrt{n}},\]
  where $\bar{x}_r$ is the mean computed on the r$^{th}$ resample and $s$ is the
  sample standard deviation from the original samples. At the end, create an
  object called \texttt{resamples.null.closer}, for example, and store the 
  resamples shifted to ensure they are consistent with the null hypotheses at the average 
  (i.e., here ensure the shifted resamples are 0 on average, corresponding
  to $t=0$, for each case). 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hldef{(boot)}

\hldef{closer} \hlkwb{<-} \hldef{dat.finch}\hlopt{$}\hldef{closer}
\hldef{further} \hlkwb{<-} \hldef{dat.finch}\hlopt{$}\hldef{further}
\hldef{diff} \hlkwb{<-} \hldef{dat.finch}\hlopt{$}\hldef{diff}

\hlcom{# Sample sizes}
\hldef{n_closer} \hlkwb{<-} \hlkwd{length}\hldef{(closer)}
\hldef{n_further} \hlkwb{<-} \hlkwd{length}\hldef{(further)}
\hldef{n_diff}    \hlkwb{<-} \hlkwd{length}\hldef{(diff)}

\hlcom{# Original standard deviations}
\hldef{s_closer} \hlkwb{<-} \hlkwd{sd}\hldef{(closer)}
\hldef{s_further} \hlkwb{<-} \hlkwd{sd}\hldef{(further)}
\hldef{s_diff} \hlkwb{<-} \hlkwd{sd}\hldef{(diff)}

\hldef{R} \hlkwb{<-} \hlnum{10000}
\hlcom{# Resample under null hypothesis: shifted to be consistent with t = 0}
\hldef{resamples.null.closer} \hlkwb{<-} \hlkwd{tibble}\hldef{(}\hlkwc{t} \hldef{=} \hlkwd{replicate}\hldef{(R, \{}
  \hldef{samp} \hlkwb{<-} \hlkwd{sample}\hldef{(closer, n_closer,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)}
  \hldef{xbar} \hlkwb{<-} \hlkwd{mean}\hldef{(samp)}
  \hldef{t} \hlkwb{<-} \hldef{(xbar} \hlopt{-} \hlkwd{mean}\hldef{(closer))} \hlopt{/} \hldef{(s_closer} \hlopt{/} \hlkwd{sqrt}\hldef{(n_closer))}  \hlcom{# shift xbar so mean is 0}
  \hlkwd{return}\hldef{(t)}
\hldef{\}))}

\hldef{resamples.null.further} \hlkwb{<-} \hlkwd{tibble}\hldef{(}\hlkwc{t} \hldef{=} \hlkwd{replicate}\hldef{(R, \{}
  \hldef{samp} \hlkwb{<-} \hlkwd{sample}\hldef{(further, n_further,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)}
  \hldef{xbar} \hlkwb{<-} \hlkwd{mean}\hldef{(samp)}
  \hldef{t} \hlkwb{<-} \hldef{(xbar} \hlopt{-} \hlkwd{mean}\hldef{(further))} \hlopt{/} \hldef{(s_further} \hlopt{/} \hlkwd{sqrt}\hldef{(n_further))}
  \hlkwd{return}\hldef{(t)}
\hldef{\}))}

\hldef{resamples.null.diff} \hlkwb{<-} \hlkwd{tibble}\hldef{(}\hlkwc{t} \hldef{=} \hlkwd{replicate}\hldef{(R, \{}
  \hldef{samp} \hlkwb{<-} \hlkwd{sample}\hldef{(diff, n_diff,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)}
  \hldef{xbar} \hlkwb{<-} \hlkwd{mean}\hldef{(samp)}
  \hldef{t} \hlkwb{<-} \hldef{(xbar} \hlopt{-} \hlkwd{mean}\hldef{(diff))} \hlopt{/} \hldef{(s_diff} \hlopt{/} \hlkwd{sqrt}\hldef{(n_diff))}
  \hlkwd{return}\hldef{(t)}
\hldef{\}))}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Compute the bootstrap $p$-value for each test using the shifted resamples. 
  How do these compare to the $t$-test $p$-values?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Observed t-statistics}
\hldef{t_obs_closer} \hlkwb{<-} \hldef{(}\hlkwd{mean}\hldef{(closer)} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(s_closer} \hlopt{/} \hlkwd{sqrt}\hldef{(n_closer))}
\hldef{t_obs_further} \hlkwb{<-} \hldef{(}\hlkwd{mean}\hldef{(further)} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(s_further} \hlopt{/} \hlkwd{sqrt}\hldef{(n_further))}
\hldef{t_obs_diff} \hlkwb{<-} \hldef{(}\hlkwd{mean}\hldef{(diff)} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(s_diff} \hlopt{/} \hlkwd{sqrt}\hldef{(n_diff))}

\hlcom{# Two-sided bootstrap p-values}
\hldef{pval_boot_closer} \hlkwb{<-} \hlkwd{mean}\hldef{(}\hlkwd{abs}\hldef{(resamples.null.closer}\hlopt{$}\hldef{t)} \hlopt{>=} \hlkwd{abs}\hldef{(t_obs_closer))}
\hldef{pval_boot_further} \hlkwb{<-} \hlkwd{mean}\hldef{(}\hlkwd{abs}\hldef{(resamples.null.further}\hlopt{$}\hldef{t)} \hlopt{>=} \hlkwd{abs}\hldef{(t_obs_further))}
\hldef{pval_boot_diff} \hlkwb{<-} \hlkwd{mean}\hldef{(}\hlkwd{abs}\hldef{(resamples.null.diff}\hlopt{$}\hldef{t)} \hlopt{>=} \hlkwd{abs}\hldef{(t_obs_diff))}

\hlcom{# Compare to t-tests}
\hldef{pval_ttest_closer} \hlkwb{<-} \hlkwd{t.test}\hldef{(closer,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{p.value}
\hldef{pval_ttest_further} \hlkwb{<-} \hlkwd{t.test}\hldef{(further,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{p.value}
\hldef{pval_ttest_diff} \hlkwb{<-} \hlkwd{t.test}\hldef{(diff,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{p.value}

\hlkwd{tibble}\hldef{(}
  \hlkwc{method} \hldef{=} \hlkwd{c}\hldef{(}\hlsng{"t-test"}\hldef{,} \hlsng{"bootstrap"}\hldef{),}
  \hlkwc{closer} \hldef{=} \hlkwd{c}\hldef{(pval_ttest_closer, pval_boot_closer),}
  \hlkwc{further} \hldef{=} \hlkwd{c}\hldef{(pval_ttest_further, pval_boot_further),}
  \hlkwc{diff} \hldef{=} \hlkwd{c}\hldef{(pval_ttest_diff, pval_boot_diff)}
\hldef{)}
\end{alltt}
\begin{verbatim}
## # A tibble: 2 x 4
##   method          closer      further         diff
##   <chr>            <dbl>        <dbl>        <dbl>
## 1 t-test    0.0000000163 0.0000000517 0.0000000104
## 2 bootstrap 0            0            0
\end{verbatim}
\end{kframe}
\end{knitrout}
Both the p values for for bootstrapping and t-test are both 0 (or close) and both or less than 0.05 the significance level.
    \item What is the 5$^{th}$ percentile of the shifted resamples under the null hypothesis? 
  Note this value approximates $t_{0.05, n-1}$. Compare these values in each case.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Compare quantile of null resamples to actual t critical values}
\hlkwd{tibble}\hldef{(}
  \hlkwc{stat} \hldef{=} \hlkwd{c}\hldef{(}\hlsng{"bootstrap"}\hldef{,} \hlsng{"t-test"}\hldef{),}
  \hlkwc{closer} \hldef{=} \hlkwd{c}\hldef{(}\hlkwd{quantile}\hldef{(resamples.null.closer}\hlopt{$}\hldef{t,} \hlnum{0.05}\hldef{),} \hlkwd{qt}\hldef{(}\hlnum{0.05}\hldef{,} \hlkwc{df} \hldef{= n_closer} \hlopt{-} \hlnum{1}\hldef{)),}
  \hlkwc{further} \hldef{=} \hlkwd{c}\hldef{(}\hlkwd{quantile}\hldef{(resamples.null.further}\hlopt{$}\hldef{t,} \hlnum{0.05}\hldef{),} \hlkwd{qt}\hldef{(}\hlnum{0.05}\hldef{,} \hlkwc{df} \hldef{= n_further} \hlopt{-} \hlnum{1}\hldef{)),}
  \hlkwc{diff} \hldef{=} \hlkwd{c}\hldef{(}\hlkwd{quantile}\hldef{(resamples.null.diff}\hlopt{$}\hldef{t,} \hlnum{0.05}\hldef{),} \hlkwd{qt}\hldef{(}\hlnum{0.05}\hldef{,} \hlkwc{df} \hldef{= n_diff} \hlopt{-} \hlnum{1}\hldef{))}
\hldef{)}
\end{alltt}
\begin{verbatim}
## # A tibble: 2 x 4
##   stat      closer further  diff
##   <chr>      <dbl>   <dbl> <dbl>
## 1 bootstrap  -1.59   -1.67 -1.57
## 2 t-test     -1.71   -1.71 -1.71
\end{verbatim}
\end{kframe}
\end{knitrout}
The values for the bootstrap method are slightly lower in the closer and difference data, and slightly higher in the further data. The values for closer and difference data is noticeably lower having values of -1.59 and -1.57 respectively compared to -1.71.
  \item Compute the bootstrap confidence intervals using the resamples. How do these 
  compare to the $t$-test confidence intervals?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Resample means}
\hldef{resample_means_closer} \hlkwb{<-} \hlkwd{replicate}\hldef{(R,} \hlkwd{mean}\hldef{(}\hlkwd{sample}\hldef{(closer, n_closer,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)))}
\hldef{resample_means_further} \hlkwb{<-} \hlkwd{replicate}\hldef{(R,} \hlkwd{mean}\hldef{(}\hlkwd{sample}\hldef{(further, n_further,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)))}
\hldef{resample_means_diff} \hlkwb{<-} \hlkwd{replicate}\hldef{(R,} \hlkwd{mean}\hldef{(}\hlkwd{sample}\hldef{(diff, n_diff,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{)))}

\hlcom{# Bootstrap CIs (percentile method)}
\hldef{ci_boot_closer} \hlkwb{<-} \hlkwd{quantile}\hldef{(resample_means_closer,} \hlkwc{probs} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.025}\hldef{,} \hlnum{0.975}\hldef{))}
\hldef{ci_boot_further} \hlkwb{<-} \hlkwd{quantile}\hldef{(resample_means_further,} \hlkwc{probs} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.025}\hldef{,} \hlnum{0.975}\hldef{))}
\hldef{ci_boot_diff} \hlkwb{<-} \hlkwd{quantile}\hldef{(resample_means_diff,} \hlkwc{probs} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.025}\hldef{,} \hlnum{0.975}\hldef{))}

\hlcom{# t-test CIs}
\hldef{ci_ttest_closer} \hlkwb{<-} \hlkwd{t.test}\hldef{(closer,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{conf.int}
\hldef{ci_ttest_further} \hlkwb{<-} \hlkwd{t.test}\hldef{(further,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{conf.int}
\hldef{ci_ttest_diff} \hlkwb{<-} \hlkwd{t.test}\hldef{(diff,} \hlkwc{mu} \hldef{=} \hlnum{0}\hldef{)}\hlopt{$}\hldef{conf.int}

\hlkwd{tibble}\hldef{(}
  \hlkwc{method} \hldef{=} \hlkwd{c}\hldef{(}\hlsng{"t-test"}\hldef{,} \hlsng{"bootstrap"}\hldef{),}
  \hlkwc{CI_closer_low} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_closer[}\hlnum{1}\hldef{], ci_boot_closer[}\hlnum{1}\hldef{]),}
  \hlkwc{CI_closer_high} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_closer[}\hlnum{2}\hldef{], ci_boot_closer[}\hlnum{2}\hldef{]),}
  \hlkwc{CI_further_low} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_further[}\hlnum{1}\hldef{], ci_boot_further[}\hlnum{1}\hldef{]),}
  \hlkwc{CI_further_high} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_further[}\hlnum{2}\hldef{], ci_boot_further[}\hlnum{2}\hldef{]),}
  \hlkwc{CI_diff_low} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_diff[}\hlnum{1}\hldef{], ci_boot_diff[}\hlnum{1}\hldef{]),}
  \hlkwc{CI_diff_high} \hldef{=} \hlkwd{c}\hldef{(ci_ttest_diff[}\hlnum{2}\hldef{], ci_boot_diff[}\hlnum{2}\hldef{])}
\hldef{)}
\end{alltt}
\begin{verbatim}
## # A tibble: 2 x 7
##   method CI_closer_low CI_closer_high CI_further_low CI_further_high CI_diff_low
##   <chr>          <dbl>          <dbl>          <dbl>           <dbl>       <dbl>
## 1 t-test         0.117          0.195         -0.257          -0.149       0.272
## 2 boots~         0.121          0.193         -0.256          -0.156       0.282
## # i 1 more variable: CI_diff_high <dbl>
\end{verbatim}
\end{kframe}
\end{knitrout}
The difference in confidence intervals between t test and bootstraps is very small. All of the differences are less than 0.01. 
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Complete the following steps to revisit the analyses from lab 11 using the
randomization procedure.
\begin{enumerate}
\item Now, consider the zebra finch data. We do not know the generating distributions
for the closer, further, and difference data, so perform the randomization procedure
  \item Compute the randomization test $p$-value for each test.
  \item Compute the randomization confidence interval by iterating over values of $\mu_0$.\\
  \textbf{Hint:} You can ``search" for the lower bound from $Q_1$ and subtracting by 0.0001, 
  and the upper bound using $Q_3$ and increasing by 0.0001. You will continue until you find 
  the first value for which the two-sided $p$-value is greater than or equal to 0.05.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Optional Question
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textbf{Optional Challenge:} In this lab, you performed resampling to 
approximate the sampling distribution of the $T$ statistic using
\[T = \frac{\bar{x}_r - 0}{s/\sqrt{n}}.\]
I'm curious whether it is better/worse/similar if we computed the statistics
using the sample standard deviation of the resamples ($s_r$), instead of the 
original sample ($s$)
  \[T = \frac{\bar{x}_r - 0}{s_r/\sqrt{n}}.\]
\begin{enumerate}
  \item Perform a simulation study to evaluate the Type I error for conducting this
hypothesis test both ways.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Parameters}
\hldef{R} \hlkwb{<-} \hlnum{1000}   \hlcom{# bootstrap resamples}
\hldef{S} \hlkwb{<-} \hlnum{1000}   \hlcom{# simulations}
\hldef{n} \hlkwb{<-} \hlnum{30}     \hlcom{# sample size}
\hldef{alpha} \hlkwb{<-} \hlnum{0.05}
\hldef{mu} \hlkwb{<-} \hlnum{0}     \hlcom{# true mean}

\hlcom{# Store rejections}
\hldef{type1_results} \hlkwb{<-} \hlkwd{tibble}\hldef{(}
  \hlkwc{reject_fixed} \hldef{=} \hlkwd{logical}\hldef{(S),}
  \hlkwc{reject_flex}  \hldef{=} \hlkwd{logical}\hldef{(S)}
\hldef{)}

\hlkwa{for} \hldef{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hldef{S) \{}
  \hldef{x} \hlkwb{<-} \hlkwd{rnorm}\hldef{(n,} \hlkwc{mean} \hldef{= mu,} \hlkwc{sd} \hldef{=} \hlnum{1}\hldef{)}
  \hldef{s} \hlkwb{<-} \hlkwd{sd}\hldef{(x)}
  \hldef{xbar} \hlkwb{<-} \hlkwd{mean}\hldef{(x)}

  \hldef{resamples} \hlkwb{<-} \hlkwd{replicate}\hldef{(R,} \hlkwd{sample}\hldef{(x, n,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{))}
  \hldef{xbars} \hlkwb{<-} \hlkwd{colMeans}\hldef{(resamples)}
  \hldef{srs} \hlkwb{<-} \hlkwd{apply}\hldef{(resamples,} \hlnum{2}\hldef{, sd)}

  \hldef{t_fixed} \hlkwb{<-} \hldef{(xbars} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(s} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}
  \hldef{t_flex} \hlkwb{<-} \hldef{(xbars} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(srs} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}

  \hldef{t_obs_fixed} \hlkwb{<-} \hldef{(xbar} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(s} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}
  \hldef{t_obs_flex} \hlkwb{<-} \hldef{(xbar} \hlopt{-} \hlnum{0}\hldef{)} \hlopt{/} \hldef{(srs} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}

  \hldef{type1_results}\hlopt{$}\hldef{reject_fixed[i]} \hlkwb{<-} \hlkwd{mean}\hldef{(}\hlkwd{abs}\hldef{(t_fixed)} \hlopt{>=} \hlkwd{abs}\hldef{(t_obs_fixed))} \hlopt{<} \hldef{alpha}
  \hldef{type1_results}\hlopt{$}\hldef{reject_flex[i]}  \hlkwb{<-} \hlkwd{mean}\hldef{(}\hlkwd{abs}\hldef{(t_flex)} \hlopt{>=} \hlkwd{abs}\hldef{(t_obs_flex))} \hlopt{<} \hldef{alpha}
\hldef{\}}

\hlcom{# Type I error rates}
\hldef{type1_results} \hlopt{%>%}
  \hlkwd{summarise}\hldef{(}
    \hlkwc{type1_fixed} \hldef{=} \hlkwd{mean}\hldef{(reject_fixed),}
    \hlkwc{type1_flex}  \hldef{=} \hlkwd{mean}\hldef{(reject_flex)}
  \hldef{)}
\end{alltt}
\begin{verbatim}
## # A tibble: 1 x 2
##   type1_fixed type1_flex
##         <dbl>      <dbl>
## 1           0          0
\end{verbatim}
\end{kframe}
\end{knitrout}
The type 1 error is 0 for both standard deviations which means they have a p value of close to 0 which makes sense given the p values we found in prior parts of the lab. 
  \item Using the same test case(s) as part (a), compute bootstrap confidence 
  intervals and assess their coverage -- how often do we `capture' the parameter
of interest?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Store CI coverage results}
\hldef{ci_results} \hlkwb{<-} \hlkwd{tibble}\hldef{(}
  \hlkwc{cover_fixed} \hldef{=} \hlkwd{logical}\hldef{(S),}
  \hlkwc{cover_flex}  \hldef{=} \hlkwd{logical}\hldef{(S)}
\hldef{)}

\hlkwa{for} \hldef{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hldef{S) \{}
  \hldef{x} \hlkwb{<-} \hlkwd{rnorm}\hldef{(n,} \hlkwc{mean} \hldef{= mu,} \hlkwc{sd} \hldef{=} \hlnum{1}\hldef{)}
  \hldef{s} \hlkwb{<-} \hlkwd{sd}\hldef{(x)}

  \hldef{resamples} \hlkwb{<-} \hlkwd{replicate}\hldef{(R,} \hlkwd{sample}\hldef{(x, n,} \hlkwc{replace} \hldef{=} \hlnum{TRUE}\hldef{))}
  \hldef{xbars} \hlkwb{<-} \hlkwd{colMeans}\hldef{(resamples)}
  \hldef{srs} \hlkwb{<-} \hlkwd{apply}\hldef{(resamples,} \hlnum{2}\hldef{, sd)}

  \hlcom{# Fixed-s approach CI}
  \hldef{ci_fixed} \hlkwb{<-} \hlkwd{quantile}\hldef{(xbars,} \hlkwc{probs} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.025}\hldef{,} \hlnum{0.975}\hldef{))}

  \hlcom{# Flex-s approach: use pivot or adjusted method}
  \hlcom{# Here: percentile method on resampled t-stats}
  \hldef{t_flex} \hlkwb{<-} \hldef{(xbars} \hlopt{-} \hlkwd{mean}\hldef{(x))} \hlopt{/} \hldef{(srs} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}
  \hldef{t_quantiles} \hlkwb{<-} \hlkwd{quantile}\hldef{(t_flex,} \hlkwc{probs} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.025}\hldef{,} \hlnum{0.975}\hldef{))}
  \hldef{ci_flex} \hlkwb{<-} \hlkwd{mean}\hldef{(x)} \hlopt{-} \hlkwd{rev}\hldef{(t_quantiles)} \hlopt{*} \hldef{(s} \hlopt{/} \hlkwd{sqrt}\hldef{(n))}

  \hldef{ci_results}\hlopt{$}\hldef{cover_fixed[i]} \hlkwb{<-} \hldef{mu} \hlopt{>=} \hldef{ci_fixed[}\hlnum{1}\hldef{]} \hlopt{&&} \hldef{mu} \hlopt{<=} \hldef{ci_fixed[}\hlnum{2}\hldef{]}
  \hldef{ci_results}\hlopt{$}\hldef{cover_flex[i]} \hlkwb{<-} \hldef{mu} \hlopt{>=} \hldef{ci_flex[}\hlnum{1}\hldef{]} \hlopt{&&} \hldef{mu} \hlopt{<=} \hldef{ci_flex[}\hlnum{2}\hldef{]}
\hldef{\}}

\hlcom{# CI coverage}
\hldef{ci_results} \hlopt{%>%}
  \hlkwd{summarise}\hldef{(}
    \hlkwc{coverage_fixed} \hldef{=} \hlkwd{mean}\hldef{(cover_fixed),}
    \hlkwc{coverage_flex}  \hldef{=} \hlkwd{mean}\hldef{(cover_flex)}
  \hldef{)}
\end{alltt}
\begin{verbatim}
## # A tibble: 1 x 2
##   coverage_fixed coverage_flex
##            <dbl>         <dbl>
## 1           0.94         0.955
\end{verbatim}
\end{kframe}
\end{knitrout}
The coverage of the flex (using the standard deviation of the resamples sr) is slightly higher than the coverage of fixed (standard deviation of original sample), both of which are around 0.95 which makes sense because the significance level is 0.05. 
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\bibliography{bibliography}
\end{document}

